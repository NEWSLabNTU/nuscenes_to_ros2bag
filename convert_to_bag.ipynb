{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 21%\r",
      "\r",
      "Reading package lists... Done\r",
      "\r\n",
      "\r",
      "Building dependency tree... 0%\r",
      "\r",
      "Building dependency tree... 0%\r",
      "\r",
      "Building dependency tree... 50%\r",
      "\r",
      "Building dependency tree... 50%\r",
      "\r",
      "Building dependency tree       \r",
      "\r\n",
      "\r",
      "Reading state information... 0%\r",
      "\r",
      "Reading state information... 0%\r",
      "\r",
      "Reading state information... Done\r",
      "\r\n",
      "git is already the newest version (1:2.25.1-1ubuntu3.8).\r\n",
      "libgeos-dev is already the newest version (3.8.0-1build1).\r\n",
      "libgl1 is already the newest version (1.3.2-1~ubuntu0.20.04.2).\r\n",
      "python3-pip is already the newest version (20.0.2-5ubuntu1.7).\r\n",
      "python3-tf2-ros is already the newest version (0.6.6-1build3).\r\n",
      "ros-noetic-foxglove-msgs is already the newest version (2.1.1-1focal.20221201.231904).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y git python3-pip python3-tf2-ros ros-noetic-foxglove-msgs libgl1 libgeos-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import rospy\n",
    "from diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus, KeyValue\n",
    "from mcap.writer import Writer, CompressionType\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from nuscenes.eval.common.utils import quaternion_yaw\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from PIL import Image\n",
    "from pypcd import pypcd\n",
    "from pyquaternion import Quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "from foxglove.CameraCalibration_pb2 import CameraCalibration\n",
    "from foxglove.CompressedImage_pb2 import CompressedImage\n",
    "from foxglove.FrameTransform_pb2 import FrameTransform\n",
    "from foxglove.Grid_pb2 import Grid\n",
    "from foxglove.ImageAnnotations_pb2 import ImageAnnotations\n",
    "from foxglove.LinePrimitive_pb2 import LinePrimitive\n",
    "from foxglove.LocationFix_pb2 import LocationFix\n",
    "from foxglove.PackedElementField_pb2 import PackedElementField\n",
    "from foxglove.PointCloud_pb2 import PointCloud\n",
    "from foxglove.PoseInFrame_pb2 import PoseInFrame\n",
    "from foxglove.PointsAnnotation_pb2 import PointsAnnotation\n",
    "from foxglove.Quaternion_pb2 import Quaternion as foxglove_Quaternion\n",
    "from foxglove.SceneUpdate_pb2 import SceneUpdate\n",
    "from foxglove.Vector3_pb2 import Vector3\n",
    "from ProtobufWriter import ProtobufWriter\n",
    "from RosmsgWriter import RosmsgWriter\n",
    "\n",
    "with open(Path(os.path.abspath('')) / \"turbomap.json\") as f:\n",
    "    TURBOMAP_DATA = np.array(json.load(f))\n",
    "\n",
    "\n",
    "# https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/can_bus/README.md#imu\n",
    "IMU_JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"linear_accel\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "                \"w\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"rotation_rate\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/can_bus/README.md#pose\n",
    "ODOM_JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"accel\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"orientation\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "                \"w\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"pos\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"rotation_rate\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "        \"vel\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\"type\": \"number\"},\n",
    "                \"y\": {\"type\": \"number\"},\n",
    "                \"z\": {\"type\": \"number\"},\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def load_bitmap(dataroot: str, map_name: str, layer_name: str) -> np.ndarray:\n",
    "    \"\"\"render bitmap map layers. Currently these are:\n",
    "    - semantic_prior: The semantic prior (driveable surface and sidewalks) mask from nuScenes 1.0.\n",
    "    - basemap: The HD lidar basemap used for localization and as general context.\n",
    "\n",
    "    :param dataroot: Path of the nuScenes dataset.\n",
    "    :param map_name: Which map out of `singapore-onenorth`, `singepore-hollandvillage`, `singapore-queenstown` and\n",
    "        'boston-seaport'.\n",
    "    :param layer_name: The type of bitmap map, `semanitc_prior` or `basemap.\n",
    "    \"\"\"\n",
    "    # Load bitmap.\n",
    "    if layer_name == \"basemap\":\n",
    "        map_path = os.path.join(dataroot, \"maps\", \"basemap\", map_name + \".png\")\n",
    "    elif layer_name == \"semantic_prior\":\n",
    "        map_hashes = {\n",
    "            \"singapore-onenorth\": \"53992ee3023e5494b90c316c183be829\",\n",
    "            \"singapore-hollandvillage\": \"37819e65e09e5547b8a3ceaefba56bb2\",\n",
    "            \"singapore-queenstown\": \"93406b464a165eaba6d9de76ca09f5da\",\n",
    "            \"boston-seaport\": \"36092f0b03a857c6a3403e25b4b7aab3\",\n",
    "        }\n",
    "        map_hash = map_hashes[map_name]\n",
    "        map_path = os.path.join(dataroot, \"maps\", map_hash + \".png\")\n",
    "    else:\n",
    "        raise Exception(\"Error: Invalid bitmap layer: %s\" % layer_name)\n",
    "\n",
    "    # Convert to numpy.\n",
    "    if os.path.exists(map_path):\n",
    "        image = np.array(Image.open(map_path).convert(\"L\"))\n",
    "    else:\n",
    "        raise Exception(\"Error: Cannot find %s %s! Please make sure that the map is correctly installed.\" % (layer_name, map_path))\n",
    "\n",
    "    # Invert semantic prior colors.\n",
    "    if layer_name == \"semantic_prior\":\n",
    "        image = image.max() - image\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "EARTH_RADIUS_METERS = 6.378137e6\n",
    "REFERENCE_COORDINATES = {\n",
    "    \"boston-seaport\": [42.336849169438615, -71.05785369873047],\n",
    "    \"singapore-onenorth\": [1.2882100868743724, 103.78475189208984],\n",
    "    \"singapore-hollandvillage\": [1.2993652317780957, 103.78217697143555],\n",
    "    \"singapore-queenstown\": [1.2782562240223188, 103.76741409301758],\n",
    "}\n",
    "\n",
    "\n",
    "def get_coordinate(ref_lat: float, ref_lon: float, bearing: float, dist: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Using a reference coordinate, extract the coordinates of another point in space given its distance and bearing\n",
    "    to the reference coordinate. For reference, please see: https://www.movable-type.co.uk/scripts/latlong.html.\n",
    "    :param ref_lat: Latitude of the reference coordinate in degrees, ie: 42.3368.\n",
    "    :param ref_lon: Longitude of the reference coordinate in degrees, ie: 71.0578.\n",
    "    :param bearing: The clockwise angle in radians between target point, reference point and the axis pointing north.\n",
    "    :param dist: The distance in meters from the reference point to the target point.\n",
    "    :return: A tuple of lat and lon.\n",
    "    \"\"\"\n",
    "    lat, lon = math.radians(ref_lat), math.radians(ref_lon)\n",
    "    angular_distance = dist / EARTH_RADIUS_METERS\n",
    "\n",
    "    target_lat = math.asin(math.sin(lat) * math.cos(angular_distance) + math.cos(lat) * math.sin(angular_distance) * math.cos(bearing))\n",
    "    target_lon = lon + math.atan2(\n",
    "        math.sin(bearing) * math.sin(angular_distance) * math.cos(lat),\n",
    "        math.cos(angular_distance) - math.sin(lat) * math.sin(target_lat),\n",
    "    )\n",
    "    return math.degrees(target_lat), math.degrees(target_lon)\n",
    "\n",
    "\n",
    "def derive_latlon(location: str, pose: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    For each pose value, extract its respective lat/lon coordinate and timestamp.\n",
    "\n",
    "    This makes the following two assumptions in order to work:\n",
    "        1. The reference coordinate for each map is in the south-western corner.\n",
    "        2. The origin of the global poses is also in the south-western corner (and identical to 1).\n",
    "    :param location: The name of the map the poses correspond to, ie: 'boston-seaport'.\n",
    "    :param poses: All nuScenes egopose dictionaries of a scene.\n",
    "    :return: A list of dicts (lat/lon coordinates and timestamps) for each pose.\n",
    "    \"\"\"\n",
    "    assert location in REFERENCE_COORDINATES.keys(), f\"Error: The given location: {location}, has no available reference.\"\n",
    "\n",
    "    reference_lat, reference_lon = REFERENCE_COORDINATES[location]\n",
    "    x, y = pose[\"translation\"][:2]\n",
    "    bearing = math.atan(x / y)\n",
    "    distance = math.sqrt(x**2 + y**2)\n",
    "    lat, lon = get_coordinate(reference_lat, reference_lon, bearing, distance)\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def get_translation(data):\n",
    "    return Vector3(x=data[\"translation\"][0], y=data[\"translation\"][1], z=data[\"translation\"][2])\n",
    "\n",
    "\n",
    "def get_rotation(data):\n",
    "    return foxglove_Quaternion(x=data[\"rotation\"][1], y=data[\"rotation\"][2], z=data[\"rotation\"][3], w=data[\"rotation\"][0])\n",
    "\n",
    "\n",
    "def get_time(data):\n",
    "    t = rospy.Time()\n",
    "    t.secs, msecs = divmod(data[\"timestamp\"], 1_000_000)\n",
    "    t.nsecs = msecs * 1000\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def get_utime(data):\n",
    "    t = rospy.Time()\n",
    "    t.secs, msecs = divmod(data[\"utime\"], 1_000_000)\n",
    "    t.nsecs = msecs * 1000\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "# See:\n",
    "# https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html\n",
    "# https://gist.github.com/mikhailov-work/ee72ba4191942acecc03fe6da94fc73f\n",
    "def turbomap(x):\n",
    "    np.clip(x, 0, 1, out=x)\n",
    "    x *= 255\n",
    "    a = x.astype(np.uint8)\n",
    "    x -= a  # compute \"f\" in place\n",
    "    b = np.minimum(254, a)\n",
    "    b += 1\n",
    "    color_a = TURBOMAP_DATA[a]\n",
    "    color_b = TURBOMAP_DATA[b]\n",
    "    color_b -= color_a\n",
    "    color_b *= x[:, np.newaxis]\n",
    "    return np.add(color_a, color_b, out=color_b)\n",
    "\n",
    "\n",
    "def get_categories(nusc, first_sample):\n",
    "    categories = set()\n",
    "    sample_lidar = first_sample\n",
    "    while sample_lidar is not None:\n",
    "        sample = nusc.get(\"sample\", sample_lidar[\"sample_token\"])\n",
    "        for annotation_id in sample[\"anns\"]:\n",
    "            ann = nusc.get(\"sample_annotation\", annotation_id)\n",
    "            categories.add(ann[\"category_name\"])\n",
    "        sample_lidar = nusc.get(\"sample_data\", sample_lidar[\"next\"]) if sample_lidar.get(\"next\") != \"\" else None\n",
    "    return categories\n",
    "\n",
    "\n",
    "PCD_TO_PACKED_ELEMENT_TYPE_MAP = {\n",
    "    (\"I\", 1): PackedElementField.INT8,\n",
    "    (\"U\", 1): PackedElementField.UINT8,\n",
    "    (\"I\", 2): PackedElementField.INT16,\n",
    "    (\"U\", 2): PackedElementField.UINT16,\n",
    "    (\"I\", 4): PackedElementField.INT32,\n",
    "    (\"U\", 4): PackedElementField.UINT32,\n",
    "    (\"F\", 4): PackedElementField.FLOAT32,\n",
    "    (\"F\", 8): PackedElementField.FLOAT64,\n",
    "}\n",
    "\n",
    "\n",
    "def get_radar(data_path, sample_data, frame_id) -> PointCloud:\n",
    "    pc_filename = data_path / sample_data[\"filename\"]\n",
    "    pc = pypcd.PointCloud.from_path(pc_filename)\n",
    "    msg = PointCloud()\n",
    "    msg.frame_id = frame_id\n",
    "    msg.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    offset = 0\n",
    "    for name, size, count, ty in zip(pc.fields, pc.size, pc.count, pc.type):\n",
    "        assert count == 1\n",
    "        msg.fields.add(name=name, offset=offset, type=PCD_TO_PACKED_ELEMENT_TYPE_MAP[(ty, size)])\n",
    "        offset += size\n",
    "\n",
    "    msg.point_stride = offset\n",
    "    msg.data = pc.pc_data.tobytes()\n",
    "    return msg\n",
    "\n",
    "\n",
    "def get_camera(data_path, sample_data, frame_id):\n",
    "    jpg_filename = data_path / sample_data[\"filename\"]\n",
    "    msg = CompressedImage()\n",
    "    msg.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    msg.frame_id = frame_id\n",
    "    msg.format = \"jpeg\"\n",
    "    with open(jpg_filename, \"rb\") as jpg_file:\n",
    "        msg.data = jpg_file.read()\n",
    "    return msg\n",
    "\n",
    "\n",
    "def get_camera_info(nusc, sample_data, frame_id):\n",
    "    calib = nusc.get(\"calibrated_sensor\", sample_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "    msg_info = CameraCalibration()\n",
    "    msg_info.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    msg_info.frame_id = frame_id\n",
    "    msg_info.height = sample_data[\"height\"]\n",
    "    msg_info.width = sample_data[\"width\"]\n",
    "    msg_info.K[:] = (calib[\"camera_intrinsic\"][r][c] for r in range(3) for c in range(3))\n",
    "    msg_info.R[:] = [1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "    msg_info.P[:] = [msg_info.K[0], msg_info.K[1], msg_info.K[2], 0, msg_info.K[3], msg_info.K[4], msg_info.K[5], 0, 0, 0, 1, 0]\n",
    "    return msg_info\n",
    "\n",
    "\n",
    "def get_lidar(data_path, sample_data, frame_id) -> PointCloud:\n",
    "    pc_filename = data_path / sample_data[\"filename\"]\n",
    "\n",
    "    with open(pc_filename, \"rb\") as pc_file:\n",
    "        msg = PointCloud()\n",
    "        msg.frame_id = frame_id\n",
    "        msg.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "        msg.fields.add(name=\"x\", offset=0, type=PackedElementField.FLOAT32),\n",
    "        msg.fields.add(name=\"y\", offset=4, type=PackedElementField.FLOAT32),\n",
    "        msg.fields.add(name=\"z\", offset=8, type=PackedElementField.FLOAT32),\n",
    "        msg.fields.add(name=\"intensity\", offset=12, type=PackedElementField.FLOAT32),\n",
    "        msg.fields.add(name=\"ring\", offset=16, type=PackedElementField.FLOAT32),\n",
    "        msg.point_stride = len(msg.fields) * 4  # 4 bytes per field\n",
    "        msg.data = pc_file.read()\n",
    "        return msg\n",
    "\n",
    "\n",
    "def get_lidar_image_annotations(nusc, sample_lidar, sample_data, frame_id):\n",
    "    # lidar image markers in camera frame\n",
    "    points, coloring, _ = nusc.explorer.map_pointcloud_to_image(\n",
    "        pointsensor_token=sample_lidar[\"token\"],\n",
    "        camera_token=sample_data[\"token\"],\n",
    "        render_intensity=True,\n",
    "    )\n",
    "    points = points.transpose()\n",
    "\n",
    "    msg = ImageAnnotations()\n",
    "    ann = msg.points.add()\n",
    "    ann.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    ann.type = PointsAnnotation.Type.POINTS\n",
    "    ann.thickness = 2\n",
    "    for p in points:\n",
    "        ann.points.add(x=p[0], y=p[1])\n",
    "    for c in turbomap(coloring):\n",
    "        ann.outline_colors.add(r=c[0], g=c[1], b=c[2], a=1)\n",
    "    return msg\n",
    "\n",
    "\n",
    "def write_boxes_image_annotations(nusc, protobuf_writer, anns, sample_data, frame_id, topic_ns, stamp):\n",
    "    # annotation boxes\n",
    "    collector = Collector()\n",
    "    _, boxes, camera_intrinsic = nusc.get_sample_data(sample_data[\"token\"])\n",
    "    for box in boxes:\n",
    "        c = np.array(nusc.explorer.get_color(box.name)) / 255.0\n",
    "        box.render(collector, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "\n",
    "    msg = ImageAnnotations()\n",
    "\n",
    "    ann = msg.points.add()\n",
    "    ann.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    ann.type = PointsAnnotation.Type.LINE_LIST\n",
    "    ann.thickness = 2\n",
    "    for p in collector.points:\n",
    "        ann.points.add(x=p[0], y=p[1])\n",
    "    for c in collector.colors:\n",
    "        ann.outline_colors.add(r=c[0], g=c[1], b=c[2], a=1)\n",
    "\n",
    "    protobuf_writer.write_message(topic_ns + \"/annotations\", msg, ann.timestamp.ToNanoseconds())\n",
    "\n",
    "\n",
    "def write_drivable_area(protobuf_writer, nusc_map, ego_pose, stamp):\n",
    "    translation = ego_pose[\"translation\"]\n",
    "    rotation = Quaternion(ego_pose[\"rotation\"])\n",
    "    yaw_radians = quaternion_yaw(rotation)\n",
    "    yaw_degrees = yaw_radians / np.pi * 180\n",
    "    patch_box = (translation[0], translation[1], 32, 32)\n",
    "    canvas_size = (patch_box[2] * 10, patch_box[3] * 10)\n",
    "\n",
    "    drivable_area = nusc_map.get_map_mask(patch_box, yaw_degrees, [\"drivable_area\"], canvas_size)[0]\n",
    "\n",
    "    msg = Grid()\n",
    "    msg.timestamp.FromNanoseconds(stamp.to_nsec())\n",
    "    msg.frame_id = \"map\"\n",
    "    msg.cell_size.x = 0.1\n",
    "    msg.cell_size.y = 0.1\n",
    "    msg.column_count = drivable_area.shape[1]\n",
    "    msg.row_stride = drivable_area.shape[1]\n",
    "    msg.cell_stride = 1\n",
    "    msg.fields.add(name=\"drivable_area\", offset=0, type=PackedElementField.UINT8)\n",
    "    msg.pose.position.x = translation[0] - (16 * math.cos(yaw_radians)) + (16 * math.sin(yaw_radians))\n",
    "    msg.pose.position.y = translation[1] - (16 * math.sin(yaw_radians)) - (16 * math.cos(yaw_radians))\n",
    "    msg.pose.position.z = 0.01  # Drivable area sits 1cm above the map\n",
    "    q = Quaternion(axis=(0, 0, 1), radians=yaw_radians)\n",
    "    msg.pose.orientation.x = q.x\n",
    "    msg.pose.orientation.y = q.y\n",
    "    msg.pose.orientation.z = q.z\n",
    "    msg.pose.orientation.w = q.w\n",
    "    msg.data = drivable_area.astype(np.uint8).tobytes()\n",
    "\n",
    "    protobuf_writer.write_message(\"/drivable_area\", msg, stamp.to_nsec())\n",
    "\n",
    "\n",
    "def get_imu_msg(imu_data):\n",
    "    timestamp = get_utime(imu_data)\n",
    "\n",
    "    msg = {\n",
    "        \"linear_accel\": {\n",
    "            \"x\": imu_data[\"linear_accel\"][0],\n",
    "            \"y\": imu_data[\"linear_accel\"][1],\n",
    "            \"z\": imu_data[\"linear_accel\"][2],\n",
    "        },\n",
    "        \"q\": {\n",
    "            \"w\": imu_data[\"q\"][0],\n",
    "            \"x\": imu_data[\"q\"][1],\n",
    "            \"y\": imu_data[\"q\"][2],\n",
    "            \"z\": imu_data[\"q\"][3],\n",
    "        },\n",
    "        \"rotation_rate\": {\n",
    "            \"x\": imu_data[\"rotation_rate\"][0],\n",
    "            \"y\": imu_data[\"rotation_rate\"][1],\n",
    "            \"z\": imu_data[\"rotation_rate\"][2],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return (timestamp, \"/imu\", json.dumps(msg).encode())\n",
    "\n",
    "\n",
    "def get_odom_msg(pose_data):\n",
    "    timestamp = get_utime(pose_data)\n",
    "\n",
    "    msg = {\n",
    "        \"accel\": {\n",
    "            \"x\": pose_data[\"accel\"][0],\n",
    "            \"y\": pose_data[\"accel\"][1],\n",
    "            \"z\": pose_data[\"accel\"][2],\n",
    "        },\n",
    "        \"orientation\": {\n",
    "            \"w\": pose_data[\"orientation\"][0],\n",
    "            \"x\": pose_data[\"orientation\"][1],\n",
    "            \"y\": pose_data[\"orientation\"][2],\n",
    "            \"z\": pose_data[\"orientation\"][3],\n",
    "        },\n",
    "        \"pos\": {\n",
    "            \"x\": pose_data[\"pos\"][0],\n",
    "            \"y\": pose_data[\"pos\"][1],\n",
    "            \"z\": pose_data[\"pos\"][2],\n",
    "        },\n",
    "        \"rotation_rate\": {\n",
    "            \"x\": pose_data[\"rotation_rate\"][0],\n",
    "            \"y\": pose_data[\"rotation_rate\"][1],\n",
    "            \"z\": pose_data[\"rotation_rate\"][2],\n",
    "        },\n",
    "        \"vel\": {\n",
    "            \"x\": pose_data[\"vel\"][0],\n",
    "            \"y\": pose_data[\"vel\"][1],\n",
    "            \"z\": pose_data[\"vel\"][2],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return (timestamp, \"/odom\", json.dumps(msg).encode())\n",
    "\n",
    "\n",
    "def get_basic_can_msg(name, diag_data):\n",
    "    values = []\n",
    "    for (key, value) in diag_data.items():\n",
    "        if key != \"utime\":\n",
    "            values.append(KeyValue(key=key, value=str(round(value, 4))))\n",
    "\n",
    "    msg = DiagnosticArray()\n",
    "    msg.header.stamp = get_utime(diag_data)\n",
    "    msg.status.append(DiagnosticStatus(name=name, level=0, message=\"OK\", values=values))\n",
    "\n",
    "    return (msg.header.stamp, \"/diagnostics\", msg)\n",
    "\n",
    "\n",
    "def get_ego_tf(ego_pose):\n",
    "    ego_tf = FrameTransform()\n",
    "    ego_tf.parent_frame_id = \"map\"\n",
    "    ego_tf.timestamp.FromMicroseconds(ego_pose[\"timestamp\"])\n",
    "    ego_tf.child_frame_id = \"base_link\"\n",
    "    ego_tf.translation.CopyFrom(get_translation(ego_pose))\n",
    "    ego_tf.rotation.CopyFrom(get_rotation(ego_pose))\n",
    "    return ego_tf\n",
    "\n",
    "\n",
    "def get_sensor_tf(nusc, sensor_id, sample_data):\n",
    "    sensor_tf = FrameTransform()\n",
    "    sensor_tf.parent_frame_id = \"base_link\"\n",
    "    sensor_tf.timestamp.FromMicroseconds(sample_data[\"timestamp\"])\n",
    "    sensor_tf.child_frame_id = sensor_id\n",
    "    calibrated_sensor = nusc.get(\"calibrated_sensor\", sample_data[\"calibrated_sensor_token\"])\n",
    "    sensor_tf.translation.CopyFrom(get_translation(calibrated_sensor))\n",
    "    sensor_tf.rotation.CopyFrom(get_rotation(calibrated_sensor))\n",
    "    return sensor_tf\n",
    "\n",
    "\n",
    "def scene_bounding_box(nusc, scene, nusc_map, padding=75.0):\n",
    "    box = [np.inf, np.inf, -np.inf, -np.inf]\n",
    "    cur_sample = nusc.get(\"sample\", scene[\"first_sample_token\"])\n",
    "    while cur_sample is not None:\n",
    "        sample_lidar = nusc.get(\"sample_data\", cur_sample[\"data\"][\"LIDAR_TOP\"])\n",
    "        ego_pose = nusc.get(\"ego_pose\", sample_lidar[\"ego_pose_token\"])\n",
    "        x, y = ego_pose[\"translation\"][:2]\n",
    "        box[0] = min(box[0], x)\n",
    "        box[1] = min(box[1], y)\n",
    "        box[2] = max(box[2], x)\n",
    "        box[3] = max(box[3], y)\n",
    "        cur_sample = nusc.get(\"sample\", cur_sample[\"next\"]) if cur_sample.get(\"next\") != \"\" else None\n",
    "    box[0] = max(box[0] - padding, 0.0)\n",
    "    box[1] = max(box[1] - padding, 0.0)\n",
    "    box[2] = min(box[2] + padding, nusc_map.canvas_edge[0]) - box[0]\n",
    "    box[3] = min(box[3] + padding, nusc_map.canvas_edge[1]) - box[1]\n",
    "    return box\n",
    "\n",
    "\n",
    "def get_scene_map(nusc, scene, nusc_map, image, stamp):\n",
    "    x, y, w, h = scene_bounding_box(nusc, scene, nusc_map)\n",
    "    img_x = int(x * 10)\n",
    "    img_y = int(y * 10)\n",
    "    img_w = int(w * 10)\n",
    "    img_h = int(h * 10)\n",
    "    img = np.flipud(image)[img_y : img_y + img_h, img_x : img_x + img_w]\n",
    "\n",
    "    # img values are 0-255\n",
    "    # convert to a color scale, 0=white and 255=black, in packed RGBA format: 0xFFFFFF00 to 0x00000000\n",
    "    img = (255 - img) * 0x01010100\n",
    "    # set alpha to 0xFF for all cells except those that are completely black\n",
    "    img[img != 0x00000000] |= 0x000000FF\n",
    "\n",
    "    msg = Grid()\n",
    "    msg.timestamp.FromNanoseconds(stamp.to_nsec())\n",
    "    msg.frame_id = \"map\"\n",
    "    msg.cell_size.x = 0.1\n",
    "    msg.cell_size.y = 0.1\n",
    "    msg.column_count = img_w\n",
    "    msg.row_stride = img_w * 4\n",
    "    msg.cell_stride = 4\n",
    "    msg.fields.add(name=\"alpha\", offset=0, type=PackedElementField.UINT8)\n",
    "    msg.fields.add(name=\"blue\", offset=1, type=PackedElementField.UINT8)\n",
    "    msg.fields.add(name=\"green\", offset=2, type=PackedElementField.UINT8)\n",
    "    msg.fields.add(name=\"red\", offset=3, type=PackedElementField.UINT8)\n",
    "    msg.pose.position.x = x\n",
    "    msg.pose.position.y = y\n",
    "    msg.pose.orientation.w = 1\n",
    "    msg.data = img.astype(\"<u4\").tobytes()\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "def rectContains(rect, point):\n",
    "    a, b, c, d = rect\n",
    "    x, y = point[:2]\n",
    "    return a <= x < a + c and b <= y < b + d\n",
    "\n",
    "\n",
    "def get_centerline_markers(nusc, scene, nusc_map, stamp):\n",
    "    pose_lists = nusc_map.discretize_centerlines(1)\n",
    "    bbox = scene_bounding_box(nusc, scene, nusc_map)\n",
    "\n",
    "    contained_pose_lists = []\n",
    "    for pose_list in pose_lists:\n",
    "        new_pose_list = []\n",
    "        for pose in pose_list:\n",
    "            if rectContains(bbox, pose):\n",
    "                new_pose_list.append(pose)\n",
    "        if len(new_pose_list) > 1:\n",
    "            contained_pose_lists.append(new_pose_list)\n",
    "\n",
    "    scene_update = SceneUpdate()\n",
    "    for i, pose_list in enumerate(contained_pose_lists):\n",
    "        entity = scene_update.entities.add()\n",
    "        entity.frame_id = \"map\"\n",
    "        entity.timestamp.FromNanoseconds(stamp.to_nsec())\n",
    "        entity.id = f\"{i}\"\n",
    "        entity.frame_locked = True\n",
    "        line = entity.lines.add()\n",
    "        line.type = LinePrimitive.Type.LINE_STRIP\n",
    "        line.thickness = 0.1\n",
    "        line.color.r = 51.0 / 255.0\n",
    "        line.color.g = 160.0 / 255.0\n",
    "        line.color.b = 44.0 / 255.0\n",
    "        line.color.a = 1.0\n",
    "        line.pose.orientation.w = 1.0\n",
    "        for pose in pose_list:\n",
    "            line.points.add(x=pose[0], y=pose[1], z=0)\n",
    "\n",
    "    return scene_update\n",
    "\n",
    "\n",
    "def find_closest_lidar(nusc, lidar_start_token, stamp_nsec):\n",
    "    candidates = []\n",
    "\n",
    "    next_lidar_token = nusc.get(\"sample_data\", lidar_start_token)[\"next\"]\n",
    "    while next_lidar_token != \"\":\n",
    "        lidar_data = nusc.get(\"sample_data\", next_lidar_token)\n",
    "        if lidar_data[\"is_key_frame\"]:\n",
    "            break\n",
    "\n",
    "        dist_abs = abs(stamp_nsec - get_time(lidar_data).to_nsec())\n",
    "        candidates.append((dist_abs, lidar_data))\n",
    "        next_lidar_token = lidar_data[\"next\"]\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "\n",
    "    return min(candidates, key=lambda x: x[0])[1]\n",
    "\n",
    "\n",
    "def get_car_scene_update(stamp) -> SceneUpdate:\n",
    "    scene_update = SceneUpdate()\n",
    "    entity = scene_update.entities.add()\n",
    "    entity.frame_id = \"base_link\"\n",
    "    entity.timestamp.FromNanoseconds(stamp)\n",
    "    entity.id = \"car\"\n",
    "    entity.frame_locked = True\n",
    "    model = entity.models.add()\n",
    "    model.pose.position.x = 1\n",
    "    model.pose.orientation.w = 1\n",
    "    model.scale.x = 1\n",
    "    model.scale.y = 1\n",
    "    model.scale.z = 1\n",
    "    model.url = \"https://assets.foxglove.dev/NuScenes_car_uncompressed.glb\"\n",
    "    return scene_update\n",
    "\n",
    "\n",
    "class Collector:\n",
    "    \"\"\"\n",
    "    Emulates the Matplotlib Axes class to collect line data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "        self.colors = []\n",
    "\n",
    "    def plot(self, xx, yy, color, linewidth):\n",
    "        x1, x2 = xx\n",
    "        y1, y2 = yy\n",
    "        self.points.append((x1, y1))\n",
    "        self.points.append((x2, y2))\n",
    "        self.colors.append(color)\n",
    "\n",
    "\n",
    "def get_num_sample_data(nusc: NuScenes, scene):\n",
    "    num_sample_data = 0\n",
    "    sample = nusc.get(\"sample\", scene[\"first_sample_token\"])\n",
    "    for sample_token in sample[\"data\"].values():\n",
    "        sample_data = nusc.get(\"sample_data\", sample_token)\n",
    "        while sample_data is not None:\n",
    "            num_sample_data += 1\n",
    "            sample_data = nusc.get(\"sample_data\", sample_data[\"next\"]) if sample_data[\"next\"] != \"\" else None\n",
    "    return num_sample_data\n",
    "\n",
    "\n",
    "def write_scene_to_mcap(nusc: NuScenes, nusc_can: NuScenesCanBus, scene, filepath):\n",
    "    scene_name = scene[\"name\"]\n",
    "    log = nusc.get(\"log\", scene[\"log_token\"])\n",
    "    location = log[\"location\"]\n",
    "    print(f'Loading map \"{location}\"')\n",
    "    data_path = Path(nusc.dataroot)\n",
    "    nusc_map = NuScenesMap(dataroot=data_path, map_name=location)\n",
    "    print(f'Loading bitmap \"{nusc_map.map_name}\"')\n",
    "    image = load_bitmap(nusc_map.dataroot, nusc_map.map_name, \"basemap\")\n",
    "    print(f\"Loaded {image.shape} bitmap\")\n",
    "    print(f\"vehicle is {log['vehicle']}\")\n",
    "\n",
    "    cur_sample = nusc.get(\"sample\", scene[\"first_sample_token\"])\n",
    "    pbar = tqdm(total=get_num_sample_data(nusc, scene), unit=\"sample_data\", desc=f\"{scene_name} Sample Data\", leave=False)\n",
    "\n",
    "    can_parsers = [\n",
    "        [nusc_can.get_messages(scene_name, \"ms_imu\"), 0, get_imu_msg],\n",
    "        [nusc_can.get_messages(scene_name, \"pose\"), 0, get_odom_msg],\n",
    "        [\n",
    "            nusc_can.get_messages(scene_name, \"steeranglefeedback\"),\n",
    "            0,\n",
    "            lambda x: get_basic_can_msg(\"Steering Angle\", x),\n",
    "        ],\n",
    "        [\n",
    "            nusc_can.get_messages(scene_name, \"vehicle_monitor\"),\n",
    "            0,\n",
    "            lambda x: get_basic_can_msg(\"Vehicle Monitor\", x),\n",
    "        ],\n",
    "        [\n",
    "            nusc_can.get_messages(scene_name, \"zoesensors\"),\n",
    "            0,\n",
    "            lambda x: get_basic_can_msg(\"Zoe Sensors\", x),\n",
    "        ],\n",
    "        [\n",
    "            nusc_can.get_messages(scene_name, \"zoe_veh_info\"),\n",
    "            0,\n",
    "            lambda x: get_basic_can_msg(\"Zoe Vehicle Info\", x),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(filepath, \"wb\") as fp:\n",
    "        print(f\"Writing to {filepath}\")\n",
    "        writer = Writer(fp, compression=CompressionType.LZ4)\n",
    "\n",
    "        imu_schema_id = writer.register_schema(name=\"IMU\", encoding=\"jsonschema\", data=json.dumps(IMU_JSON_SCHEMA).encode())\n",
    "        imu_channel_id = writer.register_channel(topic=\"/imu\", message_encoding=\"json\", schema_id=imu_schema_id)\n",
    "\n",
    "        odom_schema_id = writer.register_schema(name=\"Pose\", encoding=\"jsonschema\", data=json.dumps(ODOM_JSON_SCHEMA).encode())\n",
    "        odom_channel_id = writer.register_channel(topic=\"/odom\", message_encoding=\"json\", schema_id=odom_schema_id)\n",
    "\n",
    "        protobuf_writer = ProtobufWriter(writer)\n",
    "        rosmsg_writer = RosmsgWriter(writer)\n",
    "        writer.start(profile=\"\", library=\"nuscenes2mcap\")\n",
    "\n",
    "        writer.add_metadata(\n",
    "            \"scene-info\",\n",
    "            {\n",
    "                \"description\": scene[\"description\"],\n",
    "                \"name\": scene[\"name\"],\n",
    "                \"location\": location,\n",
    "                \"vehicle\": log[\"vehicle\"],\n",
    "                \"date_captured\": log[\"date_captured\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        stamp = get_time(\n",
    "            nusc.get(\n",
    "                \"ego_pose\",\n",
    "                nusc.get(\"sample_data\", cur_sample[\"data\"][\"LIDAR_TOP\"])[\"ego_pose_token\"],\n",
    "            )\n",
    "        )\n",
    "        map_msg = get_scene_map(nusc, scene, nusc_map, image, stamp)\n",
    "        centerlines_msg = get_centerline_markers(nusc, scene, nusc_map, stamp)\n",
    "        protobuf_writer.write_message(\"/map\", map_msg, stamp.to_nsec())\n",
    "        protobuf_writer.write_message(\"/semantic_map\", centerlines_msg, stamp.to_nsec())\n",
    "\n",
    "        while cur_sample is not None:\n",
    "            sample_lidar = nusc.get(\"sample_data\", cur_sample[\"data\"][\"LIDAR_TOP\"])\n",
    "            ego_pose = nusc.get(\"ego_pose\", sample_lidar[\"ego_pose_token\"])\n",
    "            stamp = get_time(ego_pose)\n",
    "\n",
    "            # write CAN messages to /pose, /odom, and /diagnostics\n",
    "            can_msg_events = []\n",
    "            for i in range(len(can_parsers)):\n",
    "                (can_msgs, index, msg_func) = can_parsers[i]\n",
    "                while index < len(can_msgs) and get_utime(can_msgs[index]) < stamp:\n",
    "                    can_msg_events.append(msg_func(can_msgs[index]))\n",
    "                    index += 1\n",
    "                    can_parsers[i][1] = index\n",
    "            can_msg_events.sort(key=lambda x: x[0])\n",
    "            for (msg_stamp, topic, msg) in can_msg_events:\n",
    "                if topic == \"/imu\":\n",
    "                    writer.add_message(imu_channel_id, msg_stamp.to_nsec(), msg, msg_stamp.to_nsec())\n",
    "                elif topic == \"/odom\":\n",
    "                    writer.add_message(odom_channel_id, msg_stamp.to_nsec(), msg, msg_stamp.to_nsec())\n",
    "                else:\n",
    "                    rosmsg_writer.write_message(topic, msg, msg_stamp)\n",
    "\n",
    "            # publish /tf\n",
    "            protobuf_writer.write_message(\"/tf\", get_ego_tf(ego_pose), stamp.to_nsec())\n",
    "\n",
    "            # /driveable_area occupancy grid\n",
    "            write_drivable_area(protobuf_writer, nusc_map, ego_pose, stamp)\n",
    "\n",
    "            # iterate sensors\n",
    "            for (sensor_id, sample_token) in cur_sample[\"data\"].items():\n",
    "                pbar.update(1)\n",
    "                sample_data = nusc.get(\"sample_data\", sample_token)\n",
    "                topic = \"/\" + sensor_id\n",
    "\n",
    "                # create sensor transform\n",
    "                protobuf_writer.write_message(\"/tf\", get_sensor_tf(nusc, sensor_id, sample_data), stamp.to_nsec())\n",
    "\n",
    "                # write the sensor data\n",
    "                if sample_data[\"sensor_modality\"] == \"radar\":\n",
    "                    msg = get_radar(data_path, sample_data, sensor_id)\n",
    "                    protobuf_writer.write_message(topic, msg, stamp.to_nsec())\n",
    "                elif sample_data[\"sensor_modality\"] == \"lidar\":\n",
    "                    msg = get_lidar(data_path, sample_data, sensor_id)\n",
    "                    protobuf_writer.write_message(topic, msg, stamp.to_nsec())\n",
    "                elif sample_data[\"sensor_modality\"] == \"camera\":\n",
    "                    msg = get_camera(data_path, sample_data, sensor_id)\n",
    "                    protobuf_writer.write_message(topic + \"/image_rect_compressed\", msg, stamp.to_nsec())\n",
    "                    msg = get_camera_info(nusc, sample_data, sensor_id)\n",
    "                    protobuf_writer.write_message(topic + \"/camera_info\", msg, stamp.to_nsec())\n",
    "\n",
    "                if sample_data[\"sensor_modality\"] == \"camera\":\n",
    "                    msg = get_lidar_image_annotations(nusc, sample_lidar, sample_data, sensor_id)\n",
    "                    protobuf_writer.write_message(topic + \"/lidar\", msg, stamp.to_nsec())\n",
    "                    write_boxes_image_annotations(\n",
    "                        nusc,\n",
    "                        protobuf_writer,\n",
    "                        cur_sample[\"anns\"],\n",
    "                        sample_data,\n",
    "                        sensor_id,\n",
    "                        topic,\n",
    "                        stamp,\n",
    "                    )\n",
    "\n",
    "            # publish /pose\n",
    "            pose_in_frame = PoseInFrame()\n",
    "            pose_in_frame.timestamp.FromNanoseconds(stamp.to_nsec())\n",
    "            pose_in_frame.frame_id = \"base_link\"\n",
    "            pose_in_frame.pose.orientation.w = 1\n",
    "            protobuf_writer.write_message(\"/pose\", pose_in_frame, stamp.to_nsec())\n",
    "\n",
    "            # publish /gps\n",
    "            lat, lon = derive_latlon(location, ego_pose)\n",
    "            gps = LocationFix()\n",
    "            gps.latitude = lat\n",
    "            gps.longitude = lon\n",
    "            gps.altitude = get_translation(ego_pose).z\n",
    "            protobuf_writer.write_message(\"/gps\", gps, stamp.to_nsec())\n",
    "\n",
    "            # publish /markers/annotations\n",
    "            scene_update = SceneUpdate()\n",
    "            for annotation_id in cur_sample[\"anns\"]:\n",
    "                ann = nusc.get(\"sample_annotation\", annotation_id)\n",
    "                marker_id = ann[\"instance_token\"][:4]\n",
    "                c = np.array(nusc.explorer.get_color(ann[\"category_name\"])) / 255.0\n",
    "\n",
    "                entity = scene_update.entities.add()\n",
    "                entity.frame_id = \"map\"\n",
    "                entity.timestamp.FromNanoseconds(stamp.to_nsec())\n",
    "                entity.id = marker_id\n",
    "                entity.frame_locked = True\n",
    "                metadata = entity.metadata.add()\n",
    "                metadata.key = \"category\"\n",
    "                metadata.value = ann[\"category_name\"]\n",
    "                cube = entity.cubes.add()\n",
    "                cube.pose.position.x = ann[\"translation\"][0]\n",
    "                cube.pose.position.y = ann[\"translation\"][1]\n",
    "                cube.pose.position.z = ann[\"translation\"][2]\n",
    "                cube.pose.orientation.w = ann[\"rotation\"][0]\n",
    "                cube.pose.orientation.x = ann[\"rotation\"][1]\n",
    "                cube.pose.orientation.y = ann[\"rotation\"][2]\n",
    "                cube.pose.orientation.z = ann[\"rotation\"][3]\n",
    "                cube.size.x = ann[\"size\"][1]\n",
    "                cube.size.y = ann[\"size\"][0]\n",
    "                cube.size.z = ann[\"size\"][2]\n",
    "                cube.color.r = c[0]\n",
    "                cube.color.g = c[1]\n",
    "                cube.color.b = c[2]\n",
    "                cube.color.a = 0.5\n",
    "            protobuf_writer.write_message(\"/markers/annotations\", scene_update, stamp.to_nsec())\n",
    "\n",
    "            # publish /markers/car\n",
    "            protobuf_writer.write_message(\"/markers/car\", get_car_scene_update(stamp.to_nsec()), stamp.to_nsec())\n",
    "\n",
    "            # collect all sensor frames after this sample but before the next sample\n",
    "            non_keyframe_sensor_msgs = []\n",
    "            for (sensor_id, sample_token) in cur_sample[\"data\"].items():\n",
    "                topic = \"/\" + sensor_id\n",
    "\n",
    "                next_sample_token = nusc.get(\"sample_data\", sample_token)[\"next\"]\n",
    "                while next_sample_token != \"\":\n",
    "                    next_sample_data = nusc.get(\"sample_data\", next_sample_token)\n",
    "                    # if next_sample_data['is_key_frame'] or get_time(next_sample_data).to_nsec() > next_stamp.to_nsec():\n",
    "                    #     break\n",
    "                    if next_sample_data[\"is_key_frame\"]:\n",
    "                        break\n",
    "\n",
    "                    pbar.update(1)\n",
    "                    ego_pose = nusc.get(\"ego_pose\", next_sample_data[\"ego_pose_token\"])\n",
    "                    ego_tf = get_ego_tf(ego_pose)\n",
    "                    non_keyframe_sensor_msgs.append((ego_tf.timestamp.ToNanoseconds(), \"/tf\", ego_tf))\n",
    "\n",
    "                    if next_sample_data[\"sensor_modality\"] == \"radar\":\n",
    "                        msg = get_radar(data_path, next_sample_data, sensor_id)\n",
    "                        non_keyframe_sensor_msgs.append((msg.timestamp.ToNanoseconds(), topic, msg))\n",
    "                    elif next_sample_data[\"sensor_modality\"] == \"lidar\":\n",
    "                        msg = get_lidar(data_path, next_sample_data, sensor_id)\n",
    "                        non_keyframe_sensor_msgs.append((msg.timestamp.ToNanoseconds(), topic, msg))\n",
    "                    elif next_sample_data[\"sensor_modality\"] == \"camera\":\n",
    "                        msg = get_camera(data_path, next_sample_data, sensor_id)\n",
    "                        camera_stamp_nsec = msg.timestamp.ToNanoseconds()\n",
    "                        non_keyframe_sensor_msgs.append((camera_stamp_nsec, topic + \"/image_rect_compressed\", msg))\n",
    "\n",
    "                        msg = get_camera_info(nusc, next_sample_data, sensor_id)\n",
    "                        non_keyframe_sensor_msgs.append((camera_stamp_nsec, topic + \"/camera_info\", msg))\n",
    "\n",
    "                        closest_lidar = find_closest_lidar(nusc, cur_sample[\"data\"][\"LIDAR_TOP\"], camera_stamp_nsec)\n",
    "                        if closest_lidar is not None:\n",
    "                            msg = get_lidar_image_annotations(nusc, closest_lidar, next_sample_data, sensor_id)\n",
    "                            non_keyframe_sensor_msgs.append(\n",
    "                                (\n",
    "                                    msg.points[0].timestamp.ToNanoseconds(),\n",
    "                                    topic + \"/lidar\",\n",
    "                                    msg,\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                    next_sample_token = next_sample_data[\"next\"]\n",
    "\n",
    "            # sort and publish the non-keyframe sensor msgs\n",
    "            non_keyframe_sensor_msgs.sort(key=lambda x: x[0])\n",
    "            for (timestamp, topic, msg) in non_keyframe_sensor_msgs:\n",
    "                if hasattr(msg, \"header\"):\n",
    "                    rosmsg_writer.write_message(topic, msg, msg.header.stamp)\n",
    "                else:\n",
    "                    protobuf_writer.write_message(topic, msg, timestamp)\n",
    "\n",
    "            # move to the next sample\n",
    "            cur_sample = nusc.get(\"sample\", cur_sample[\"next\"]) if cur_sample.get(\"next\") != \"\" else None\n",
    "\n",
    "        pbar.close()\n",
    "        writer.finish()\n",
    "        print(f\"Finished writing {filepath}\")\n",
    "\n",
    "\n",
    "def convert_all(\n",
    "    output_dir: Path,\n",
    "    name: str,\n",
    "    nusc: NuScenes,\n",
    "    nusc_can: NuScenesCanBus,\n",
    "    selected_scenes,\n",
    "):\n",
    "    nusc.list_scenes()\n",
    "    for scene in nusc.scene:\n",
    "        scene_name = scene[\"name\"]\n",
    "        if selected_scenes is not None and scene_name not in selected_scenes:\n",
    "            continue\n",
    "        mcap_name = f\"NuScenes-{name}-{scene_name}.mcap\"\n",
    "        write_scene_to_mcap(nusc, nusc_can, scene, output_dir / mcap_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.882 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.2 seconds.\n",
      "======\n",
      "scene-0061, Parked truck, construction, intersectio... [18-07-24 03:28:47]   19s, singapore-onenorth, #anns:4622\n",
      "scene-0103, Many peds right, wait for turning car, ... [18-08-01 19:26:43]   19s, boston-seaport, #anns:2046\n",
      "scene-0655, Parking lot, parked cars, jaywalker, be... [18-08-27 15:51:32]   20s, boston-seaport, #anns:2332\n",
      "scene-0553, Wait at intersection, bicycle, large tr... [18-08-28 20:48:16]   20s, boston-seaport, #anns:1950\n",
      "scene-0757, Arrive at busy intersection, bus, wait ... [18-08-30 19:25:08]   20s, boston-seaport, #anns:592\n",
      "scene-0796, Scooter, peds on sidewalk, bus, cars, t... [18-10-02 02:52:24]   20s, singapore-queensto, #anns:708\n",
      "scene-0916, Parking lot, bicycle rack, parked bicyc... [18-10-08 07:37:13]   20s, singapore-queensto, #anns:2387\n",
      "scene-1077, Night, big street, bus stop, high speed... [18-11-21 11:39:27]   20s, singapore-hollandv, #anns:890\n",
      "scene-1094, Night, after rain, many peds, PMD, ped ... [18-11-21 11:47:27]   19s, singapore-hollandv, #anns:1762\n",
      "scene-1100, Night, peds in sidewalk, peds cross cro... [18-11-21 11:49:47]   19s, singapore-hollandv, #anns:935\n",
      "Loading map \"singapore-onenorth\"\n",
      "Loading bitmap \"singapore-onenorth\"\n",
      "Loaded (20250, 15856) bitmap\n",
      "vehicle is n015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "scene-0061 Sample Data:   0%|                  | 0/2963 [00:00<?, ?sample_data/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to experimental_out/NuScenes-v1.0-mini-scene-0061.mcap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nuscenes/map_expansion/map_api.py:1823: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
      "/usr/local/lib/python3.8/dist-packages/nuscenes/map_expansion/map_api.py:1824: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n",
      "scene-0061 Sample Data:  18%|█▍      | 526/2963 [02:53<03:39, 11.10sample_data/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nusc \u001b[38;5;241m=\u001b[39m NuScenes(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1.0-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataroot\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m nusc_can \u001b[38;5;241m=\u001b[39m NuScenesCanBus(dataroot\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mconvert_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./experimental_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv1.0-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnusc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnusc_can\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 927\u001b[0m, in \u001b[0;36mconvert_all\u001b[0;34m(output_dir, name, nusc, nusc_can, selected_scenes)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    926\u001b[0m mcap_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNuScenes-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mcap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 927\u001b[0m \u001b[43mwrite_scene_to_mcap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnusc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnusc_can\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmcap_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 887\u001b[0m, in \u001b[0;36mwrite_scene_to_mcap\u001b[0;34m(nusc, nusc_can, scene, filepath)\u001b[0m\n\u001b[1;32m    885\u001b[0m     closest_lidar \u001b[38;5;241m=\u001b[39m find_closest_lidar(nusc, cur_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIDAR_TOP\u001b[39m\u001b[38;5;124m\"\u001b[39m], camera_stamp_nsec)\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closest_lidar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[43mget_lidar_image_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnusc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosest_lidar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_sample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m         non_keyframe_sensor_msgs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    889\u001b[0m             (\n\u001b[1;32m    890\u001b[0m                 msg\u001b[38;5;241m.\u001b[39mpoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mToNanoseconds(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[1;32m    894\u001b[0m         )\n\u001b[1;32m    896\u001b[0m next_sample_token \u001b[38;5;241m=\u001b[39m next_sample_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 338\u001b[0m, in \u001b[0;36mget_lidar_image_annotations\u001b[0;34m(nusc, sample_lidar, sample_data, frame_id)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_lidar_image_annotations\u001b[39m(nusc, sample_lidar, sample_data, frame_id):\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# lidar image markers in camera frame\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     points, coloring, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnusc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_pointcloud_to_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpointsensor_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_lidar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcamera_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_intensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     points \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    345\u001b[0m     msg \u001b[38;5;241m=\u001b[39m ImageAnnotations()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nuscenes/nuscenes.py:884\u001b[0m, in \u001b[0;36mNuScenesExplorer.map_pointcloud_to_image\u001b[0;34m(self, pointsensor_token, camera_token, min_dist, render_intensity, show_lidarseg, filter_lidarseg_labels, lidarseg_preds_bin_path, show_panoptic)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# Points live in the point sensor frame. So they need to be transformed via global to the image plane.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m cs_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnusc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalibrated_sensor\u001b[39m\u001b[38;5;124m'\u001b[39m, pointsensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalibrated_sensor_token\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 884\u001b[0m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuaternion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcs_record\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrotation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotation_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m pc\u001b[38;5;241m.\u001b[39mtranslate(np\u001b[38;5;241m.\u001b[39marray(cs_record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Second step: transform from ego to the global frame.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nuscenes/utils/data_classes.py:173\u001b[0m, in \u001b[0;36mPointCloud.rotate\u001b[0;34m(self, rot_matrix)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrotate\u001b[39m(\u001b[38;5;28mself\u001b[39m, rot_matrix: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Applies a rotation.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    :param rot_matrix: <np.float: 3, 3>. Rotation matrix.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints[:\u001b[38;5;241m3\u001b[39m, :] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrot_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nusc = NuScenes(version=\"v1.0-mini\", dataroot=\"./data\", verbose=True)\n",
    "nusc_can = NuScenesCanBus(dataroot=\"./data\")\n",
    "convert_all(Path(\"./experimental_out\"), \"v1.0-mini\", nusc, nusc_can, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
